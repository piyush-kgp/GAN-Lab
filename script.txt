
..Greetings, encourage questions..

5 - when i = 784, p(x) denotes p(image occur)
    Not much diff between PixelRNN and PixelCNN

8 - AE has no mean and sigma, simply an Enc and a Dec.
    Arch - We calculate the mu and sigma for each minibatch. Then sample z from N(mu, sigma) and pass decoder through that z.
           We have 2 losses, 1 for reconstruction and 1 forcing the z distribution towards unit Gaussian.
9 - In an E-D network, we want to find theta to maximize P(x)
    In the eqn, P theta x/z means prob dist of x conditional on z. which is the decoder model.
    P theta z means distribution of z.
    1st term is Gausian distribution. 2nd term, we dont know distribution. We know at certain z, an image can be created. But what about those z where there is no x.
    Hence intractable.


    In 2nd eqn,
    LHS is prob of image when z is sampled from gaussian.
    1st term in RHS is the reconstruction loss.
    KLD is estimation of how well q phi is similar to z. Now z is Gaussian. So this means KL is minimized when q phi thing is as close to gaussian as possible.

11 - vae_loss is as discussed.

12 - x and y denote the dimensions of latent variables, color of dots represent image label of x.
    Notice
      latent variables of images of a label stick together.
      TL - 4 and 9 are mixed which means features are preserved. This idea that features are preserved is imp.
      Blur in reconstructed images.

13 - probs with DBN, VAE, GAN-2player,

14 - G 100 dim gaussian noise -> 28x28 image using upsampling, conv2dtranspose etc.
     D 28x28 image -> 1 label denotes prob of image coming from training data.

15 - Eqn published by Goodfellow in his now legendary(say most cited) GAN paper
    What this means is that both G and D have opposite optimzation objectives.
    log(D(x)) is the probability of of a real image being correctly labelled as real.
    log(1-D(G(z))) is the probabilityof a fake image being correctly labelled as fake.
    Thus maximization of V means D performance maximization.
    Minimization of V means G performance maximization.


18 - We change the rules of the game for the competition between D and G.
     Also we train_on_batch G and D separately.
     1st eqn trains D.
     If you are confused why are we maximizing things instead of minimizing them since you must have written optimizer.minimize(); the answer is in that situation also you're maximizing the expectation of your detector working correctly. For ex -
     If you were to compare 1st equation to a hotdog detector, x would be hotdog, G(z) is not hotdog.
     And then you calculate the crossentropy; which is -V, which then you minimize.
     Ofc you dont need a 2nd eqn.

     2nd eqn trains G and wants to fools the D.

19 - Left wing trains D, Right wing trains G, we keep D non-trainable during right wing.



Audience questions
15 - Which term corresponds to P(Hotdog) and P(Not Hotdog)?
What prevents the network from giving the same fake image everytime?
