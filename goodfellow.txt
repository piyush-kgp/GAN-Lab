Generative Modelling
To take a collection of training examples and form some representation of a probability distribution that
can explain where those training examples came from.

To do this you can either infer a density function that describes the prob dist that generated them.

GANs fall under 2nd category. PixelCNN and PixelRNN are variants of 1st category. Actually they model conditional probability rather than raw probability.

(A) Why study generative models?
High dimensional probability distribution is really one of the most basic stuff in ML. So anything that helps us manipulate that is important to any ML practitioner. So it finds application in many ML approaches.
1. For example -  Generative models can be used to simulate possible futures for RL. One is you can train your agent in a simulated environment rather than needing to build an environment by hand. Adv. of course is that parallelizable across machines and mistakes are not costly in reality. For example you could simulate 3-D objects for a robot learning to grasp.
Work by Chelsea Finn.
2. For Input-Output model, it can handle missing output. Although not much work has been done here.
3. Data augmentation in Semi-supervised learning - Very few labelled inputs are available.
5. Realistic generation tasks themselves - example - Deoldify, next frame prediction, Domain transfer horse 2 zebra or bw to color, style transfer, superresoulution, reverse image segmentation, text to image, face synthesis and so on.
Neural Photo editing
Multi-modal outputs like next frame prediction.
Image to image translation- isola et al. - map to satellite, invert edge detection


(B, C) How do generative models work?
VAE places a lower bound on log likelihood and maximizes that lower bound.
Basic frameowork consists of Generator and Disciminator.
The most common analogy that you'll find in blogs is counterfeiters and police.

Q. What prevents the generator from constantly generating the same fake image.
One of the failure modes is actually the generator having little diversity among its generated images.
Q. When to use VAE vs GANs?
VAE will work better to maximize likelihood of p(x). GAN will work better to generate images. This can be seen in the loss function.

There is a blog by Chris Olah (link in ref.) to what's behind the checkerboard pattern in GAN-generated images. It has to do with the deconv. operation, specifically that each cell in output is not visited equal number of times by the kernel during the op.


* Notice that the gradient is guaranteed to be non-zero.
Notice the beauty of the cross-entropy loss is that the gradient os the loss is (logarithmically) proportional to how wrong the prediction is; either it has gradient and is making a mistake or it lacks gradient and is perfect.

FIgURE.

Both of these are monotonically decreasing but have steep gradients in different places.
Rather than G minimizing prob of D being correct, we maximize prob of D being wrong in the AM.

DCGAN - Radford et al., FAIR. Batch Normalization was not common when the first GAN paper came out.

Vector space arithmetic similar to word embedding algebra.


(D) Tips and Tricks
Labels improve sample quality. Class conditional GAN or CGAN

One sided label smoothing vs weight decay - Does not reduce classification accuracy; only reduce confidence.

Batch Norm in Generator can cause strong intra-batch correlation. A solution is Reference Batch Norm.

Balancing G and D:
If D gets really good at reject G-gen. samples, G doesn't have a gradient anymore. But when this happens do not make D less powerful, use tricks like one-sided label smoothing and non-saurating cost.



Research Frontiers
Confronting the non-convergence problem.

Mode Collapse - Introduce mini-batch features. Salimans et al 2016.

Problems with counting, perspective, global structure.

Another solution to mode collapse is unrolled gans. metz et al. 2016. Backprop through k updates of the discriminator. This is actually very interesting. Generator is actually looking into the future.

Evaluation of Generative Models is another challenge altogether. A note on the evaluation of generative models. Theis et at.

Discrete outputs - G has to be differentiable remember?

Semi-supervised learning with GANs. Instead of D having label real, fake it now has labels real cat, real dog, fake. Salimans et al 2016. Since it learns to reject fake data, it helps the D regularize very well, thus enabling it to do classification with very small number of examples.

Learning to make the latent code interpretable. InfoGAN

RL connections - Actor Critic Pfau and Vinyl
Imitation learning, Inverse RL.

Other games in AI - most famous is the adversarial examples.
